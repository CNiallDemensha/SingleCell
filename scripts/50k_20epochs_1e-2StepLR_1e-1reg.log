Running --> python3 /nas/longleaf/home/athreya/gmm/scripts/train.py --cfg_file=params.json
{'att_dims': 128,
 'batch_size': 2000,
 'clip': 0.01,
 'data_root': '/nas/longleaf/home/athreya/gmm/data/',
 'epochs': 20,
 'exp_dir': '/nas/longleaf/home/athreya/gmm/model/',
 'k': 14,
 'lr': 0.01,
 'resume_from_epoch': -1,
 'seed': 1234,
 'tau': 1,
 'weight_decay': 0}
Device -: cuda:0
Starting training
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [44.86906433105469, 224.59689331054688, 44.86906433105469]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [44.04934310913086, 228.69281005859375, 44.04934310913086]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [43.348323822021484, 236.31393432617188, 43.348323822021484]
Average train epoch loss for epoch 1 = 44.350697708129886
Epoch 1 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 1 - Test Loss = 43.009681701660156, Vscore = 0.4349881535364771, ARI = 0.3116381522921912, AMI = 0.4153553007273636
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [42.576026916503906, 227.4185028076172, 44.85021209716797]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [42.22977828979492, 197.22293090820312, 44.20200729370117]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [41.7823600769043, 163.66354370117188, 43.41899490356445]
Average train epoch loss for epoch 2 = 42.22530727386474
Epoch 2 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 2 - Test Loss = 41.38093872070313, Vscore = 0.5124974139068396, ARI = 0.39594061951554177, AMI = 0.48801160982557934
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [41.10801696777344, 102.97016143798828, 42.137718200683594]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [40.58300018310547, 83.25533294677734, 41.41555404663086]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [39.98234558105469, 71.12602233886719, 40.693607330322266]
Average train epoch loss for epoch 3 = 40.607573699951175
Epoch 3 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 3 - Test Loss = 39.78846664428711, Vscore = 0.5512876240996953, ARI = 0.45935871604396494, AMI = 0.5349096265891334
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [39.293216705322266, 59.635292053222656, 39.88956832885742]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [39.07021713256836, 56.80462646484375, 39.63826370239258]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [38.80643844604492, 54.939579010009766, 39.3558349609375]
Average train epoch loss for epoch 4 = 39.0757942199707
Epoch 4 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 4 - Test Loss = 38.335394287109374, Vscore = 0.5875519565342587, ARI = 0.5156346045066512, AMI = 0.579685165854692
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [37.908714294433594, 52.541290283203125, 38.43412780761719]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [37.818241119384766, 51.77941131591797, 38.336036682128906]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [37.76530075073242, 51.186283111572266, 38.277164459228516]
Average train epoch loss for epoch 5 = 37.72772750854492
Epoch 5 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 5 - Test Loss = 37.11173324584961, Vscore = 0.6199585022832532, ARI = 0.5731958573121082, AMI = 0.6121149539936389
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [36.79482650756836, 50.19114685058594, 37.29673767089844]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [36.4444580078125, 49.89045715332031, 36.943363189697266]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [36.189659118652344, 49.77510070800781, 36.687408447265625]
Average train epoch loss for epoch 6 = 36.56490058898926
Epoch 6 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 6 - Test Loss = 35.95675506591797, Vscore = 0.6606631620193338, ARI = 0.6321957163225872, AMI = 0.6483501767486269
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [35.689815521240234, 49.35400390625, 36.18335723876953]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [35.39361572265625, 49.22705078125, 35.885887145996094]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [35.217811584472656, 49.2714958190918, 35.71052551269531]
Average train epoch loss for epoch 7 = 35.41374549865723
Epoch 7 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 7 - Test Loss = 34.88754119873047, Vscore = 0.6842339570256046, ARI = 0.6610258985408779, AMI = 0.6728345752733483
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [34.73678207397461, 50.162261962890625, 35.2384033203125]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [34.474369049072266, 50.838218688964844, 34.982749938964844]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [34.17049026489258, 51.201942443847656, 34.68251037597656]
Average train epoch loss for epoch 8 = 34.40409393310547
Epoch 8 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 8 - Test Loss = 33.9166015625, Vscore = 0.6987022845953855, ARI = 0.6761028568650788, AMI = 0.6857890777058416
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [33.88960266113281, 51.250213623046875, 34.402103424072266]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [33.65137481689453, 51.18462371826172, 34.1632194519043]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [33.40126037597656, 51.09705352783203, 33.9122314453125]
Average train epoch loss for epoch 9 = 33.55148849487305
Epoch 9 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 9 - Test Loss = 33.19086837768555, Vscore = 0.6960753855053409, ARI = 0.6758057674005211, AMI = 0.6860692758870602
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [32.79236602783203, 50.6446418762207, 33.29881286621094]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [33.069618225097656, 50.02812957763672, 33.56990051269531]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [32.58942794799805, 49.403564453125, 33.08346176147461]
Average train epoch loss for epoch 10 = 32.895881271362306
Epoch 10 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 10 - Test Loss = 32.61072692871094, Vscore = 0.6997962331742306, ARI = 0.6811555551933026, AMI = 0.6945530988346329
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [32.64387893676758, 49.0367431640625, 33.134246826171875]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [32.31682586669922, 49.07152557373047, 32.80754089355469]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [32.25663757324219, 49.28538131713867, 32.74949264526367]
Average train epoch loss for epoch 11 = 32.34041538238525
Epoch 11 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 11 - Test Loss = 32.01726455688477, Vscore = 0.7139497147062479, ARI = 0.6949453048949584, AMI = 0.7127239436105742
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [31.547712326049805, 49.765167236328125, 32.04536437988281]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [31.36355209350586, 49.83161926269531, 31.861867904663086]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [31.59610939025879, 49.881553649902344, 32.09492492675781]
Average train epoch loss for epoch 12 = 31.65757637023926
Epoch 12 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 12 - Test Loss = 31.38023910522461, Vscore = 0.7288787289740072, ARI = 0.6961446044791261, AMI = 0.723786858441084
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [31.117904663085938, 50.295654296875, 31.620861053466797]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [31.2564697265625, 50.52849578857422, 31.761754989624023]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [30.636743545532227, 50.61830139160156, 31.142927169799805]
Average train epoch loss for epoch 13 = 31.146009063720705
Epoch 13 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 13 - Test Loss = 30.94798927307129, Vscore = 0.726184220259888, ARI = 0.6892760629972879, AMI = 0.719098924705332
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [30.9218692779541, 50.63240051269531, 31.428194046020508]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [30.963598251342773, 50.63252639770508, 31.46992301940918]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [30.924457550048828, 50.64488220214844, 31.430906295776367]
Average train epoch loss for epoch 14 = 30.78740167617798
Epoch 14 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 14 - Test Loss = 30.677138900756837, Vscore = 0.7265924402223263, ARI = 0.6880012713024637, AMI = 0.7171436737818211
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [30.774967193603516, 50.951751708984375, 31.28448486328125]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [30.63129425048828, 51.32420349121094, 31.1445369720459]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [30.45957374572754, 51.490234375, 30.974475860595703]
Average train epoch loss for epoch 15 = 30.508262062072752
Epoch 15 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 15 - Test Loss = 30.368222427368163, Vscore = 0.7340594195405652, ARI = 0.7029090066869663, AMI = 0.72381003426194
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [30.827739715576172, 51.52119827270508, 31.34295082092285]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [30.166635513305664, 51.51858901977539, 30.681821823120117]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [30.003334045410156, 51.51531219482422, 30.51848793029785]
Average train epoch loss for epoch 16 = 30.27158327102661
Epoch 16 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 16 - Test Loss = 30.21603927612305, Vscore = 0.7463201527051179, ARI = 0.7202384187432871, AMI = 0.7366197261199657
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [30.542587280273438, 51.51406478881836, 31.057727813720703]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [30.21521759033203, 51.51849365234375, 30.73040199279785]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [29.936973571777344, 51.52765655517578, 30.45224952697754]
Average train epoch loss for epoch 17 = 30.1232666015625
Epoch 17 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 17 - Test Loss = 30.084169387817383, Vscore = 0.7440305806685413, ARI = 0.7155005688375322, AMI = 0.7322260273291595
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [29.970325469970703, 51.5662956237793, 30.48598861694336]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [29.61079978942871, 51.62582015991211, 30.127058029174805]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [30.08230972290039, 51.720882415771484, 30.599517822265625]
Average train epoch loss for epoch 18 = 30.026218128204345
Epoch 18 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 18 - Test Loss = 29.982579040527344, Vscore = 0.7406685225404449, ARI = 0.7134683657049381, AMI = 0.7281360700140843
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [29.940937042236328, 52.216304779052734, 30.46310043334961]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [29.71084213256836, 52.46139907836914, 30.235456466674805]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [30.33136749267578, 52.61334991455078, 30.857500076293945]
Average train epoch loss for epoch 19 = 29.941995906829835
Epoch 19 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 19 - Test Loss = 29.90621337890625, Vscore = 0.7354192374628429, ARI = 0.7032509246558611, AMI = 0.7222855489343307
Iteration 10000 / 40000 - Loss (nll, l1_loss, total) = [29.76495361328125, 53.31964874267578, 30.29814910888672]
Iteration 20000 / 40000 - Loss (nll, l1_loss, total) = [29.805707931518555, 53.88896560668945, 30.34459686279297]
Iteration 30000 / 40000 - Loss (nll, l1_loss, total) = [29.380207061767578, 54.29755401611328, 29.92318344116211]
Average train epoch loss for epoch 20 = 29.846077823638915
Epoch 20 - Eval mode
Test label shape = (10000,), Predicted label shape = (10000,)
Epoch 20 - Test Loss = 29.76625289916992, Vscore = 0.7322297203087188, ARI = 0.6936012650701061, AMI = 0.7182987923014038
Training finished
Epoch level Train_Loss, Test_Loss, V_Score, ARI, AMI -:
[44.350697708129886, 42.22530727386474, 40.607573699951175, 39.0757942199707, 37.72772750854492, 36.56490058898926, 35.41374549865723, 34.40409393310547, 33.55148849487305, 32.895881271362306, 32.34041538238525, 31.65757637023926, 31.146009063720705, 30.78740167617798, 30.508262062072752, 30.27158327102661, 30.1232666015625, 30.026218128204345, 29.941995906829835, 29.846077823638915]
[43.009681701660156, 41.38093872070313, 39.78846664428711, 38.335394287109374, 37.11173324584961, 35.95675506591797, 34.88754119873047, 33.9166015625, 33.19086837768555, 32.61072692871094, 32.01726455688477, 31.38023910522461, 30.94798927307129, 30.677138900756837, 30.368222427368163, 30.21603927612305, 30.084169387817383, 29.982579040527344, 29.90621337890625, 29.76625289916992]
[0.4349881535364771, 0.5124974139068396, 0.5512876240996953, 0.5875519565342587, 0.6199585022832532, 0.6606631620193338, 0.6842339570256046, 0.6987022845953855, 0.6960753855053409, 0.6997962331742306, 0.7139497147062479, 0.7288787289740072, 0.726184220259888, 0.7265924402223263, 0.7340594195405652, 0.7463201527051179, 0.7440305806685413, 0.7406685225404449, 0.7354192374628429, 0.7322297203087188]
[0.3116381522921912, 0.39594061951554177, 0.45935871604396494, 0.5156346045066512, 0.5731958573121082, 0.6321957163225872, 0.6610258985408779, 0.6761028568650788, 0.6758057674005211, 0.6811555551933026, 0.6949453048949584, 0.6961446044791261, 0.6892760629972879, 0.6880012713024637, 0.7029090066869663, 0.7202384187432871, 0.7155005688375322, 0.7134683657049381, 0.7032509246558611, 0.6936012650701061]
[0.4153553007273636, 0.48801160982557934, 0.5349096265891334, 0.579685165854692, 0.6121149539936389, 0.6483501767486269, 0.6728345752733483, 0.6857890777058416, 0.6860692758870602, 0.6945530988346329, 0.7127239436105742, 0.723786858441084, 0.719098924705332, 0.7171436737818211, 0.72381003426194, 0.7366197261199657, 0.7322260273291595, 0.7281360700140843, 0.7222855489343307, 0.7182987923014038]
Performing k-means clustering to 14 components of 40000 samples in dimension 32/32 ...
K-means took: 3.986833095550537 sec
KMeans V_Score, ARI, AMI -: 0.3579120416125808, 0.31368524625499505, 0.34609332918163993

Model alpha

tensor([[0.3379, 0.3444, 0.3295, 0.3180, 0.3365, 0.3264, 0.3422, 0.3158, 0.3327,
         0.3191, 0.3465, 0.3468, 0.3229, 0.3136, 0.3344, 0.3442, 0.3439, 0.3174,
         0.3404, 0.3371, 0.3362, 0.3440, 0.3449, 0.3188, 0.3454, 0.3177, 0.3464,
         0.3405, 0.3211, 0.3168, 0.3413, 0.3234],
        [0.7290, 0.3188, 0.2794, 0.2864, 0.7716, 0.6821, 0.7334, 0.3005, 0.3102,
         0.2857, 0.3118, 0.3406, 0.2891, 0.2858, 0.7065, 0.7218, 0.7651, 0.2799,
         0.7569, 0.2702, 0.3549, 0.2987, 0.3001, 0.2706, 0.7455, 0.2646, 0.6786,
         0.3007, 0.3596, 0.3140, 0.7298, 0.6829],
        [0.3390, 0.3387, 0.3213, 0.3218, 0.3278, 0.3143, 0.3303, 0.7023, 0.3351,
         0.3084, 0.3467, 0.3428, 0.6690, 0.3188, 0.3381, 0.3462, 0.3156, 0.3161,
         0.3304, 0.3447, 0.3466, 0.3512, 0.3432, 0.3428, 0.3458, 0.3148, 0.3445,
         0.3531, 0.3253, 0.3474, 0.3340, 0.3234],
        [0.6476, 0.3259, 0.3490, 0.2958, 0.3068, 0.3003, 0.7334, 0.3187, 0.3310,
         0.2968, 0.3440, 0.3472, 0.3335, 0.3002, 0.3400, 0.3354, 0.3243, 0.3194,
         0.3141, 0.3554, 0.3441, 0.3276, 0.3361, 0.3008, 0.3310, 0.3267, 0.3180,
         0.3301, 0.3101, 0.3267, 0.3095, 0.3229],
        [0.3396, 0.3392, 0.2933, 0.2861, 0.3492, 0.3160, 0.6703, 0.3134, 0.3024,
         0.2941, 0.5897, 0.3487, 0.3060, 0.2896, 0.3067, 0.3286, 0.3105, 0.3201,
         0.3043, 0.7255, 0.3170, 0.7307, 0.7185, 0.3322, 0.3198, 0.6986, 0.3363,
         0.3137, 0.3112, 0.7218, 0.2798, 0.3075],
        [0.7075, 0.3402, 0.2996, 0.3020, 0.3115, 0.6971, 0.3019, 0.7335, 0.3214,
         0.3019, 0.3686, 0.3312, 0.7394, 0.6942, 0.7050, 0.3305, 0.6757, 0.3092,
         0.3077, 0.3197, 0.3542, 0.3179, 0.3330, 0.3438, 0.6855, 0.2798, 0.3394,
         0.7249, 0.3239, 0.5980, 0.3021, 0.3178],
        [0.3324, 0.3231, 0.2985, 0.2802, 0.6572, 0.3242, 0.7351, 0.2957, 0.3326,
         0.2888, 0.3458, 0.3244, 0.2966, 0.2896, 0.3173, 0.3230, 0.2857, 0.3110,
         0.2925, 0.3680, 0.3217, 0.3126, 0.3305, 0.3079, 0.3188, 0.2861, 0.3273,
         0.3251, 0.3062, 0.3156, 0.2775, 0.2956],
        [0.3333, 0.3433, 0.3240, 0.3419, 0.3545, 0.3216, 0.3418, 0.3477, 0.6979,
         0.4591, 0.3470, 0.3326, 0.3252, 0.3172, 0.3448, 0.3441, 0.3458, 0.3145,
         0.3714, 0.3222, 0.3442, 0.3537, 0.3445, 0.3195, 0.3477, 0.3167, 0.3485,
         0.3456, 0.3288, 0.3227, 0.3807, 0.3143],
        [0.3380, 0.3457, 0.3205, 0.3196, 0.3392, 0.3309, 0.3333, 0.3333, 0.3301,
         0.3189, 0.3465, 0.3253, 0.3356, 0.3128, 0.3459, 0.3455, 0.3226, 0.3173,
         0.3422, 0.3216, 0.3422, 0.3416, 0.3449, 0.3199, 0.3463, 0.3215, 0.3456,
         0.3421, 0.3177, 0.3313, 0.3421, 0.3440],
        [0.6506, 0.3444, 0.3199, 0.3282, 0.3307, 0.3368, 0.6838, 0.3307, 0.3315,
         0.3342, 0.3446, 0.3450, 0.3288, 0.3063, 0.3427, 0.3406, 0.3346, 0.3245,
         0.3287, 0.3463, 0.3289, 0.3367, 0.3408, 0.3162, 0.3344, 0.3175, 0.3437,
         0.3369, 0.3146, 0.3382, 0.3302, 0.3383],
        [0.3400, 0.3357, 0.3294, 0.3171, 0.3165, 0.3485, 0.3571, 0.3412, 0.3232,
         0.3210, 0.3455, 0.3486, 0.3132, 0.2974, 0.3400, 0.3408, 0.2962, 0.3082,
         0.3221, 0.3350, 0.3465, 0.3237, 0.3383, 0.3337, 0.3371, 0.3189, 0.3441,
         0.7150, 0.3438, 0.3302, 0.3217, 0.3318],
        [0.7304, 0.3387, 0.2934, 0.2815, 0.2959, 0.2985, 0.7358, 0.3170, 0.3318,
         0.2866, 0.3443, 0.3473, 0.2931, 0.2914, 0.3189, 0.3264, 0.2804, 0.3123,
         0.2768, 0.6742, 0.3235, 0.3137, 0.3311, 0.3055, 0.3209, 0.3136, 0.3383,
         0.3245, 0.3097, 0.3150, 0.2870, 0.2972],
        [0.7345, 0.3366, 0.2769, 0.2685, 0.2690, 0.6986, 0.7406, 0.2757, 0.3124,
         0.2681, 0.3469, 0.3352, 0.2977, 0.2617, 0.3046, 0.3066, 0.2876, 0.2910,
         0.2792, 0.3322, 0.3018, 0.2899, 0.6830, 0.7075, 0.3127, 0.2655, 0.3106,
         0.2974, 0.2992, 0.2978, 0.2824, 0.2846],
        [0.3445, 0.3558, 0.3445, 0.7624, 0.2978, 0.7047, 0.7255, 0.2990, 0.2922,
         0.7607, 0.3462, 0.6357, 0.2872, 0.2775, 0.3301, 0.3226, 0.2965, 0.3052,
         0.2823, 0.2986, 0.3503, 0.2999, 0.3210, 0.2918, 0.3096, 0.2888, 0.6672,
         0.3036, 0.3522, 0.3095, 0.2936, 0.6795]], device='cuda:0',
       grad_fn=<SigmoidBackward>)
