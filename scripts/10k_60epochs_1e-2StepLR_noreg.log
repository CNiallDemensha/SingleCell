Running --> python3 /nas/longleaf/home/athreya/gmm/scripts/train.py --cfg_file=params.json
{'att_dims': 128,
 'batch_size': 500,
 'clip': 0.01,
 'data_root': '/nas/longleaf/home/athreya/gmm/data/',
 'epochs': 60,
 'exp_dir': '/nas/longleaf/home/athreya/gmm/model/',
 'k': 14,
 'lr': 0.01,
 'resume_from_epoch': -1,
 'seed': 1234,
 'tau': 1,
 'weight_decay': 0}
Device -: cuda:0
Starting training
Iteration 1000 / 8000 - Loss = 45.569149017333984
Iteration 2000 / 8000 - Loss = 45.373191833496094
Iteration 3000 / 8000 - Loss = 44.53731155395508
Iteration 4000 / 8000 - Loss = 44.256927490234375
Iteration 5000 / 8000 - Loss = 43.97742462158203
Iteration 6000 / 8000 - Loss = 43.783512115478516
Iteration 7000 / 8000 - Loss = 43.55883026123047
Average train epoch loss for epoch 1 = 44.61981773376465
Epoch 1 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 1 - Vscore = 0.3618294868423858, ARI = 0.21674685317346645, AMI = 0.3411988390377493
Iteration 1000 / 8000 - Loss = 42.97166442871094
Iteration 2000 / 8000 - Loss = 43.8489875793457
Iteration 3000 / 8000 - Loss = 43.11069107055664
Iteration 4000 / 8000 - Loss = 42.37175369262695
Iteration 5000 / 8000 - Loss = 42.93684005737305
Iteration 6000 / 8000 - Loss = 42.04545974731445
Iteration 7000 / 8000 - Loss = 42.332881927490234
Average train epoch loss for epoch 2 = 42.83835220336914
Epoch 2 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 2 - Vscore = 0.4714971198337818, ARI = 0.3364090736875596, AMI = 0.433606745951033
Iteration 1000 / 8000 - Loss = 41.562522888183594
Iteration 2000 / 8000 - Loss = 41.523921966552734
Iteration 3000 / 8000 - Loss = 40.932987213134766
Iteration 4000 / 8000 - Loss = 41.09028244018555
Iteration 5000 / 8000 - Loss = 41.00353240966797
Iteration 6000 / 8000 - Loss = 40.837703704833984
Iteration 7000 / 8000 - Loss = 40.682220458984375
Average train epoch loss for epoch 3 = 41.14787316322327
Epoch 3 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 3 - Vscore = 0.500078155070635, ARI = 0.3498781270531325, AMI = 0.4559990288869092
Iteration 1000 / 8000 - Loss = 39.8357048034668
Iteration 2000 / 8000 - Loss = 39.792205810546875
Iteration 3000 / 8000 - Loss = 39.56950378417969
Iteration 4000 / 8000 - Loss = 39.42493438720703
Iteration 5000 / 8000 - Loss = 39.60847091674805
Iteration 6000 / 8000 - Loss = 38.89639663696289
Iteration 7000 / 8000 - Loss = 38.812744140625
Average train epoch loss for epoch 4 = 39.5243284702301
Epoch 4 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 4 - Vscore = 0.5342629081957665, ARI = 0.37798738131479875, AMI = 0.4902277678680698
Iteration 1000 / 8000 - Loss = 38.421146392822266
Iteration 2000 / 8000 - Loss = 37.95671463012695
Iteration 3000 / 8000 - Loss = 38.19097137451172
Iteration 4000 / 8000 - Loss = 38.01358413696289
Iteration 5000 / 8000 - Loss = 38.077476501464844
Iteration 6000 / 8000 - Loss = 38.018821716308594
Iteration 7000 / 8000 - Loss = 37.687461853027344
Average train epoch loss for epoch 5 = 38.040292263031006
Epoch 5 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 5 - Vscore = 0.5866289512787091, ARI = 0.42667275615541883, AMI = 0.5424568996253448
Iteration 1000 / 8000 - Loss = 36.89904022216797
Iteration 2000 / 8000 - Loss = 36.98143005371094
Iteration 3000 / 8000 - Loss = 37.03750991821289
Iteration 4000 / 8000 - Loss = 36.789100646972656
Iteration 5000 / 8000 - Loss = 36.218658447265625
Iteration 6000 / 8000 - Loss = 36.067142486572266
Iteration 7000 / 8000 - Loss = 36.32200241088867
Average train epoch loss for epoch 6 = 36.671791791915894
Epoch 6 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 6 - Vscore = 0.6197420973201849, ARI = 0.47004525988476364, AMI = 0.5766506509995158
Iteration 1000 / 8000 - Loss = 35.69325256347656
Iteration 2000 / 8000 - Loss = 35.54615020751953
Iteration 3000 / 8000 - Loss = 35.50381088256836
Iteration 4000 / 8000 - Loss = 35.44222640991211
Iteration 5000 / 8000 - Loss = 35.1932373046875
Iteration 6000 / 8000 - Loss = 35.338844299316406
Iteration 7000 / 8000 - Loss = 34.916709899902344
Average train epoch loss for epoch 7 = 35.50427722930908
Epoch 7 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 7 - Vscore = 0.6517846811882854, ARI = 0.5023825447339653, AMI = 0.6087214731766533
Iteration 1000 / 8000 - Loss = 34.776634216308594
Iteration 2000 / 8000 - Loss = 34.7676887512207
Iteration 3000 / 8000 - Loss = 34.628047943115234
Iteration 4000 / 8000 - Loss = 34.432743072509766
Iteration 5000 / 8000 - Loss = 34.238948822021484
Iteration 6000 / 8000 - Loss = 34.475433349609375
Iteration 7000 / 8000 - Loss = 33.90812301635742
Average train epoch loss for epoch 8 = 34.46940732002258
Epoch 8 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 8 - Vscore = 0.6733365189466051, ARI = 0.5175051544535477, AMI = 0.63089544149184
Iteration 1000 / 8000 - Loss = 33.33378219604492
Iteration 2000 / 8000 - Loss = 33.99363708496094
Iteration 3000 / 8000 - Loss = 33.91108703613281
Iteration 4000 / 8000 - Loss = 33.62395477294922
Iteration 5000 / 8000 - Loss = 33.22483825683594
Iteration 6000 / 8000 - Loss = 33.59804153442383
Iteration 7000 / 8000 - Loss = 33.27608108520508
Average train epoch loss for epoch 9 = 33.529078006744385
Epoch 9 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 9 - Vscore = 0.6798800343593567, ARI = 0.5200278903898246, AMI = 0.6390594861294816
Iteration 1000 / 8000 - Loss = 32.53129196166992
Iteration 2000 / 8000 - Loss = 32.97567367553711
Iteration 3000 / 8000 - Loss = 32.95193099975586
Iteration 4000 / 8000 - Loss = 32.428924560546875
Iteration 5000 / 8000 - Loss = 32.851829528808594
Iteration 6000 / 8000 - Loss = 32.45018005371094
Iteration 7000 / 8000 - Loss = 32.495113372802734
Average train epoch loss for epoch 10 = 32.65729594230652
Epoch 10 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 10 - Vscore = 0.6844441911183624, ARI = 0.5141807925890026, AMI = 0.6435290823878768
Iteration 1000 / 8000 - Loss = 32.111263275146484
Iteration 2000 / 8000 - Loss = 31.932708740234375
Iteration 3000 / 8000 - Loss = 32.46201705932617
Iteration 4000 / 8000 - Loss = 31.61405372619629
Iteration 5000 / 8000 - Loss = 32.24776840209961
Iteration 6000 / 8000 - Loss = 31.438039779663086
Iteration 7000 / 8000 - Loss = 31.701875686645508
Average train epoch loss for epoch 11 = 31.868276357650757
Epoch 11 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 11 - Vscore = 0.6908909188532137, ARI = 0.508874212532295, AMI = 0.6503432957099283
Iteration 1000 / 8000 - Loss = 31.178152084350586
Iteration 2000 / 8000 - Loss = 31.360801696777344
Iteration 3000 / 8000 - Loss = 31.403759002685547
Iteration 4000 / 8000 - Loss = 31.110368728637695
Iteration 5000 / 8000 - Loss = 30.830198287963867
Iteration 6000 / 8000 - Loss = 31.082639694213867
Iteration 7000 / 8000 - Loss = 31.02747917175293
Average train epoch loss for epoch 12 = 31.183314085006714
Epoch 12 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 12 - Vscore = 0.694371843442731, ARI = 0.5073234698994905, AMI = 0.6539662510474074
Iteration 1000 / 8000 - Loss = 30.545137405395508
Iteration 2000 / 8000 - Loss = 30.824403762817383
Iteration 3000 / 8000 - Loss = 30.254941940307617
Iteration 4000 / 8000 - Loss = 30.93427085876465
Iteration 5000 / 8000 - Loss = 30.576919555664062
Iteration 6000 / 8000 - Loss = 30.415061950683594
Iteration 7000 / 8000 - Loss = 30.021099090576172
Average train epoch loss for epoch 13 = 30.56778347492218
Epoch 13 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 13 - Vscore = 0.6946071489381981, ARI = 0.49639150066301496, AMI = 0.6542555797493497
Iteration 1000 / 8000 - Loss = 30.175310134887695
Iteration 2000 / 8000 - Loss = 30.213727951049805
Iteration 3000 / 8000 - Loss = 29.984012603759766
Iteration 4000 / 8000 - Loss = 30.440954208374023
Iteration 5000 / 8000 - Loss = 30.01091194152832
Iteration 6000 / 8000 - Loss = 29.66916847229004
Iteration 7000 / 8000 - Loss = 29.426254272460938
Average train epoch loss for epoch 14 = 30.002180337905884
Epoch 14 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 14 - Vscore = 0.6958150907666806, ARI = 0.4942241931746379, AMI = 0.6556367300147984
Iteration 1000 / 8000 - Loss = 29.818920135498047
Iteration 2000 / 8000 - Loss = 29.10552215576172
Iteration 3000 / 8000 - Loss = 29.38456153869629
Iteration 4000 / 8000 - Loss = 29.59104347229004
Iteration 5000 / 8000 - Loss = 29.06405258178711
Iteration 6000 / 8000 - Loss = 29.250205993652344
Iteration 7000 / 8000 - Loss = 29.082212448120117
Average train epoch loss for epoch 15 = 29.486168146133423
Epoch 15 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 15 - Vscore = 0.7026769710027627, ARI = 0.4956276352502448, AMI = 0.6628236846995792
Iteration 1000 / 8000 - Loss = 29.08550453186035
Iteration 2000 / 8000 - Loss = 29.389822006225586
Iteration 3000 / 8000 - Loss = 28.841257095336914
Iteration 4000 / 8000 - Loss = 29.242029190063477
Iteration 5000 / 8000 - Loss = 29.08623504638672
Iteration 6000 / 8000 - Loss = 28.262954711914062
Iteration 7000 / 8000 - Loss = 29.017515182495117
Average train epoch loss for epoch 16 = 29.052592754364014
Epoch 16 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 16 - Vscore = 0.7036022050729589, ARI = 0.4946983670841489, AMI = 0.6636454936098857
Iteration 1000 / 8000 - Loss = 28.64498519897461
Iteration 2000 / 8000 - Loss = 28.942106246948242
Iteration 3000 / 8000 - Loss = 28.637157440185547
Iteration 4000 / 8000 - Loss = 28.481571197509766
Iteration 5000 / 8000 - Loss = 29.192138671875
Iteration 6000 / 8000 - Loss = 28.77655601501465
Iteration 7000 / 8000 - Loss = 28.04279899597168
Average train epoch loss for epoch 17 = 28.653364658355713
Epoch 17 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 17 - Vscore = 0.7034057799078066, ARI = 0.49433127185537756, AMI = 0.6629679443073905
Iteration 1000 / 8000 - Loss = 28.92178726196289
Iteration 2000 / 8000 - Loss = 27.969654083251953
Iteration 3000 / 8000 - Loss = 27.62006378173828
Iteration 4000 / 8000 - Loss = 28.273656845092773
Iteration 5000 / 8000 - Loss = 28.127347946166992
Iteration 6000 / 8000 - Loss = 28.175127029418945
Iteration 7000 / 8000 - Loss = 28.221376419067383
Average train epoch loss for epoch 18 = 28.310531497001648
Epoch 18 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 18 - Vscore = 0.7073034587139532, ARI = 0.4980553587490815, AMI = 0.6663532205548528
Iteration 1000 / 8000 - Loss = 27.824399948120117
Iteration 2000 / 8000 - Loss = 27.641510009765625
Iteration 3000 / 8000 - Loss = 28.44401741027832
Iteration 4000 / 8000 - Loss = 27.692289352416992
Iteration 5000 / 8000 - Loss = 28.272489547729492
Iteration 6000 / 8000 - Loss = 28.233421325683594
Iteration 7000 / 8000 - Loss = 28.794050216674805
Average train epoch loss for epoch 19 = 28.032676577568054
Epoch 19 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 19 - Vscore = 0.7090947357567222, ARI = 0.5009104616789678, AMI = 0.667702258199486
Iteration 1000 / 8000 - Loss = 27.583345413208008
Iteration 2000 / 8000 - Loss = 27.76229476928711
Iteration 3000 / 8000 - Loss = 28.049009323120117
Iteration 4000 / 8000 - Loss = 27.65983009338379
Iteration 5000 / 8000 - Loss = 27.6507568359375
Iteration 6000 / 8000 - Loss = 27.38119888305664
Iteration 7000 / 8000 - Loss = 27.27593231201172
Average train epoch loss for epoch 20 = 27.759687662124634
Epoch 20 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 20 - Vscore = 0.7027200241452135, ARI = 0.4980491592818407, AMI = 0.6600487747437122
Iteration 1000 / 8000 - Loss = 27.713647842407227
Iteration 2000 / 8000 - Loss = 27.445770263671875
Iteration 3000 / 8000 - Loss = 27.203203201293945
Iteration 4000 / 8000 - Loss = 27.667896270751953
Iteration 5000 / 8000 - Loss = 27.4388427734375
Iteration 6000 / 8000 - Loss = 27.549081802368164
Iteration 7000 / 8000 - Loss = 27.203365325927734
Average train epoch loss for epoch 21 = 27.49682593345642
Epoch 21 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 21 - Vscore = 0.7033528221313655, ARI = 0.4960279803695325, AMI = 0.660352882684789
Iteration 1000 / 8000 - Loss = 26.461544036865234
Iteration 2000 / 8000 - Loss = 27.34860610961914
Iteration 3000 / 8000 - Loss = 27.459529876708984
Iteration 4000 / 8000 - Loss = 27.1986026763916
Iteration 5000 / 8000 - Loss = 27.194942474365234
Iteration 6000 / 8000 - Loss = 26.993593215942383
Iteration 7000 / 8000 - Loss = 26.973121643066406
Average train epoch loss for epoch 22 = 27.298112511634827
Epoch 22 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 22 - Vscore = 0.702606774088673, ARI = 0.49655374758673754, AMI = 0.6587871731678517
Iteration 1000 / 8000 - Loss = 26.969240188598633
Iteration 2000 / 8000 - Loss = 26.969676971435547
Iteration 3000 / 8000 - Loss = 27.06079864501953
Iteration 4000 / 8000 - Loss = 27.043832778930664
Iteration 5000 / 8000 - Loss = 27.784761428833008
Iteration 6000 / 8000 - Loss = 27.23009490966797
Iteration 7000 / 8000 - Loss = 27.355031967163086
Average train epoch loss for epoch 23 = 27.14396059513092
Epoch 23 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 23 - Vscore = 0.7032359072214717, ARI = 0.4957810381300904, AMI = 0.6589893560819308
Iteration 1000 / 8000 - Loss = 26.82436752319336
Iteration 2000 / 8000 - Loss = 27.308212280273438
Iteration 3000 / 8000 - Loss = 26.802867889404297
Iteration 4000 / 8000 - Loss = 27.44095802307129
Iteration 5000 / 8000 - Loss = 26.92011070251465
Iteration 6000 / 8000 - Loss = 27.23824691772461
Iteration 7000 / 8000 - Loss = 27.127656936645508
Average train epoch loss for epoch 24 = 26.992637872695923
Epoch 24 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 24 - Vscore = 0.7046663641731108, ARI = 0.49435895058851326, AMI = 0.6595216781462884
Iteration 1000 / 8000 - Loss = 26.71173095703125
Iteration 2000 / 8000 - Loss = 27.087806701660156
Iteration 3000 / 8000 - Loss = 27.15256690979004
Iteration 4000 / 8000 - Loss = 26.54715347290039
Iteration 5000 / 8000 - Loss = 26.326814651489258
Iteration 6000 / 8000 - Loss = 27.53621482849121
Iteration 7000 / 8000 - Loss = 26.753536224365234
Average train epoch loss for epoch 25 = 26.875083446502686
Epoch 25 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 25 - Vscore = 0.700744177016709, ARI = 0.4912618989151982, AMI = 0.6547579166983216
Iteration 1000 / 8000 - Loss = 26.731313705444336
Iteration 2000 / 8000 - Loss = 26.173124313354492
Iteration 3000 / 8000 - Loss = 26.867855072021484
Iteration 4000 / 8000 - Loss = 26.807113647460938
Iteration 5000 / 8000 - Loss = 26.724143981933594
Iteration 6000 / 8000 - Loss = 26.26270866394043
Iteration 7000 / 8000 - Loss = 26.455265045166016
Average train epoch loss for epoch 26 = 26.65231204032898
Epoch 26 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 26 - Vscore = 0.7021980172511908, ARI = 0.4926434808213699, AMI = 0.6566668274703067
Iteration 1000 / 8000 - Loss = 26.65403938293457
Iteration 2000 / 8000 - Loss = 27.15325927734375
Iteration 3000 / 8000 - Loss = 26.84711456298828
Iteration 4000 / 8000 - Loss = 26.580677032470703
Iteration 5000 / 8000 - Loss = 26.561328887939453
Iteration 6000 / 8000 - Loss = 26.364648818969727
Iteration 7000 / 8000 - Loss = 25.957738876342773
Average train epoch loss for epoch 27 = 26.562967538833618
Epoch 27 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 27 - Vscore = 0.7034279270422903, ARI = 0.4906261920041116, AMI = 0.6569253489532058
Iteration 1000 / 8000 - Loss = 26.043363571166992
Iteration 2000 / 8000 - Loss = 26.495868682861328
Iteration 3000 / 8000 - Loss = 26.242767333984375
Iteration 4000 / 8000 - Loss = 26.75782585144043
Iteration 5000 / 8000 - Loss = 26.23164176940918
Iteration 6000 / 8000 - Loss = 26.51453399658203
Iteration 7000 / 8000 - Loss = 27.16387176513672
Average train epoch loss for epoch 28 = 26.516089916229248
Epoch 28 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 28 - Vscore = 0.7011030479723017, ARI = 0.48701603512673614, AMI = 0.6546323210699019
Iteration 1000 / 8000 - Loss = 26.34319305419922
Iteration 2000 / 8000 - Loss = 26.61083221435547
Iteration 3000 / 8000 - Loss = 26.601716995239258
Iteration 4000 / 8000 - Loss = 26.632516860961914
Iteration 5000 / 8000 - Loss = 26.71565055847168
Iteration 6000 / 8000 - Loss = 26.61369514465332
Iteration 7000 / 8000 - Loss = 27.020122528076172
Average train epoch loss for epoch 29 = 26.469815135002136
Epoch 29 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 29 - Vscore = 0.7001491904180508, ARI = 0.48729425263896664, AMI = 0.6536646737767187
Iteration 1000 / 8000 - Loss = 26.18880271911621
Iteration 2000 / 8000 - Loss = 26.875289916992188
Iteration 3000 / 8000 - Loss = 25.975759506225586
Iteration 4000 / 8000 - Loss = 26.40644073486328
Iteration 5000 / 8000 - Loss = 25.838605880737305
Iteration 6000 / 8000 - Loss = 26.058435440063477
Iteration 7000 / 8000 - Loss = 26.512466430664062
Average train epoch loss for epoch 30 = 26.41976499557495
Epoch 30 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 30 - Vscore = 0.7049663406455517, ARI = 0.49098039400856464, AMI = 0.657805699819362
Iteration 1000 / 8000 - Loss = 26.66591453552246
Iteration 2000 / 8000 - Loss = 26.461894989013672
Iteration 3000 / 8000 - Loss = 26.218856811523438
Iteration 4000 / 8000 - Loss = 26.01569175720215
Iteration 5000 / 8000 - Loss = 26.92363166809082
Iteration 6000 / 8000 - Loss = 26.536041259765625
Iteration 7000 / 8000 - Loss = 26.339649200439453
Average train epoch loss for epoch 31 = 26.385303735733032
Epoch 31 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 31 - Vscore = 0.7035996579255634, ARI = 0.48812391914702086, AMI = 0.6560401246886695
Iteration 1000 / 8000 - Loss = 26.431705474853516
Iteration 2000 / 8000 - Loss = 26.318012237548828
Iteration 3000 / 8000 - Loss = 25.954139709472656
Iteration 4000 / 8000 - Loss = 26.580997467041016
Iteration 5000 / 8000 - Loss = 27.011240005493164
Iteration 6000 / 8000 - Loss = 26.529769897460938
Iteration 7000 / 8000 - Loss = 26.733543395996094
Average train epoch loss for epoch 32 = 26.350269198417664
Epoch 32 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 32 - Vscore = 0.7049629512828006, ARI = 0.48808376650023955, AMI = 0.6575429857686182
Iteration 1000 / 8000 - Loss = 26.31292724609375
Iteration 2000 / 8000 - Loss = 26.38766098022461
Iteration 3000 / 8000 - Loss = 26.45377540588379
Iteration 4000 / 8000 - Loss = 25.49774169921875
Iteration 5000 / 8000 - Loss = 26.630903244018555
Iteration 6000 / 8000 - Loss = 26.361845016479492
Iteration 7000 / 8000 - Loss = 25.855857849121094
Average train epoch loss for epoch 33 = 26.31752061843872
Epoch 33 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 33 - Vscore = 0.7041796652564563, ARI = 0.4886435622767034, AMI = 0.6568600643865556
Iteration 1000 / 8000 - Loss = 25.85637664794922
Iteration 2000 / 8000 - Loss = 26.069242477416992
Iteration 3000 / 8000 - Loss = 26.512441635131836
Iteration 4000 / 8000 - Loss = 26.085247039794922
Iteration 5000 / 8000 - Loss = 26.67679786682129
Iteration 6000 / 8000 - Loss = 26.86492919921875
Iteration 7000 / 8000 - Loss = 25.913450241088867
Average train epoch loss for epoch 34 = 26.286338090896606
Epoch 34 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 34 - Vscore = 0.705742154558113, ARI = 0.49009452113099033, AMI = 0.6579420996091335
Iteration 1000 / 8000 - Loss = 25.28473472595215
Iteration 2000 / 8000 - Loss = 25.868282318115234
Iteration 3000 / 8000 - Loss = 26.508628845214844
Iteration 4000 / 8000 - Loss = 26.311321258544922
Iteration 5000 / 8000 - Loss = 26.790857315063477
Iteration 6000 / 8000 - Loss = 26.171144485473633
Iteration 7000 / 8000 - Loss = 26.366958618164062
Average train epoch loss for epoch 35 = 26.264025330543518
Epoch 35 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 35 - Vscore = 0.7080280688912498, ARI = 0.49297554703534724, AMI = 0.6607181720902334
Iteration 1000 / 8000 - Loss = 25.771583557128906
Iteration 2000 / 8000 - Loss = 26.266469955444336
Iteration 3000 / 8000 - Loss = 26.08028793334961
Iteration 4000 / 8000 - Loss = 26.07603645324707
Iteration 5000 / 8000 - Loss = 26.287029266357422
Iteration 6000 / 8000 - Loss = 26.286897659301758
Iteration 7000 / 8000 - Loss = 26.97079849243164
Average train epoch loss for epoch 36 = 26.238107681274414
Epoch 36 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 36 - Vscore = 0.7059668125420938, ARI = 0.4923504556890125, AMI = 0.6579768759117404
Iteration 1000 / 8000 - Loss = 25.6586971282959
Iteration 2000 / 8000 - Loss = 26.207216262817383
Iteration 3000 / 8000 - Loss = 26.878070831298828
Iteration 4000 / 8000 - Loss = 26.441781997680664
Iteration 5000 / 8000 - Loss = 26.25750732421875
Iteration 6000 / 8000 - Loss = 26.372575759887695
Iteration 7000 / 8000 - Loss = 26.188913345336914
Average train epoch loss for epoch 37 = 26.216638922691345
Epoch 37 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 37 - Vscore = 0.7049312238309631, ARI = 0.4891101471526743, AMI = 0.6571791711813519
Iteration 1000 / 8000 - Loss = 25.987892150878906
Iteration 2000 / 8000 - Loss = 25.487653732299805
Iteration 3000 / 8000 - Loss = 26.566062927246094
Iteration 4000 / 8000 - Loss = 26.314783096313477
Iteration 5000 / 8000 - Loss = 26.53084373474121
Iteration 6000 / 8000 - Loss = 26.45002555847168
Iteration 7000 / 8000 - Loss = 26.676742553710938
Average train epoch loss for epoch 38 = 26.1919105052948
Epoch 38 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 38 - Vscore = 0.7058227614333411, ARI = 0.49241796254716796, AMI = 0.6579214105570415
Iteration 1000 / 8000 - Loss = 25.78406524658203
Iteration 2000 / 8000 - Loss = 25.76053237915039
Iteration 3000 / 8000 - Loss = 26.351699829101562
Iteration 4000 / 8000 - Loss = 25.492809295654297
Iteration 5000 / 8000 - Loss = 25.92827606201172
Iteration 6000 / 8000 - Loss = 26.47348976135254
Iteration 7000 / 8000 - Loss = 25.699304580688477
Average train epoch loss for epoch 39 = 26.182292222976685
Epoch 39 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 39 - Vscore = 0.7069397813995092, ARI = 0.4931878023878558, AMI = 0.6591875047415608
Iteration 1000 / 8000 - Loss = 26.0752010345459
Iteration 2000 / 8000 - Loss = 26.05331039428711
Iteration 3000 / 8000 - Loss = 25.672483444213867
Iteration 4000 / 8000 - Loss = 26.314809799194336
Iteration 5000 / 8000 - Loss = 26.31538200378418
Iteration 6000 / 8000 - Loss = 26.238611221313477
Iteration 7000 / 8000 - Loss = 25.920875549316406
Average train epoch loss for epoch 40 = 26.14860248565674
Epoch 40 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 40 - Vscore = 0.7035604963262062, ARI = 0.4909154917878728, AMI = 0.6553264604108198
Iteration 1000 / 8000 - Loss = 25.84446907043457
Iteration 2000 / 8000 - Loss = 25.774520874023438
Iteration 3000 / 8000 - Loss = 26.609111785888672
Iteration 4000 / 8000 - Loss = 25.78498649597168
Iteration 5000 / 8000 - Loss = 26.267637252807617
Iteration 6000 / 8000 - Loss = 26.38422966003418
Iteration 7000 / 8000 - Loss = 25.808670043945312
Average train epoch loss for epoch 41 = 26.13693141937256
Epoch 41 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 41 - Vscore = 0.7051454403957585, ARI = 0.490911879429626, AMI = 0.6564782032733119
Iteration 1000 / 8000 - Loss = 25.731578826904297
Iteration 2000 / 8000 - Loss = 25.744922637939453
Iteration 3000 / 8000 - Loss = 26.241201400756836
Iteration 4000 / 8000 - Loss = 25.839458465576172
Iteration 5000 / 8000 - Loss = 25.97206687927246
Iteration 6000 / 8000 - Loss = 26.356611251831055
Iteration 7000 / 8000 - Loss = 26.521154403686523
Average train epoch loss for epoch 42 = 26.116631269454956
Epoch 42 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 42 - Vscore = 0.7043378570657856, ARI = 0.4903653465058634, AMI = 0.6557595871287303
Iteration 1000 / 8000 - Loss = 25.975637435913086
Iteration 2000 / 8000 - Loss = 25.89800453186035
Iteration 3000 / 8000 - Loss = 26.923538208007812
Iteration 4000 / 8000 - Loss = 26.663726806640625
Iteration 5000 / 8000 - Loss = 26.61785316467285
Iteration 6000 / 8000 - Loss = 25.88704490661621
Iteration 7000 / 8000 - Loss = 26.735441207885742
Average train epoch loss for epoch 43 = 26.100979328155518
Epoch 43 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 43 - Vscore = 0.7025939196658003, ARI = 0.4897631619694284, AMI = 0.6541288630073397
Iteration 1000 / 8000 - Loss = 26.17852783203125
Iteration 2000 / 8000 - Loss = 25.676599502563477
Iteration 3000 / 8000 - Loss = 25.641361236572266
Iteration 4000 / 8000 - Loss = 26.19643783569336
Iteration 5000 / 8000 - Loss = 26.158897399902344
Iteration 6000 / 8000 - Loss = 26.270009994506836
Iteration 7000 / 8000 - Loss = 26.581296920776367
Average train epoch loss for epoch 44 = 26.093749046325684
Epoch 44 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 44 - Vscore = 0.7048822482260255, ARI = 0.49045563237068296, AMI = 0.6564465685792996
Iteration 1000 / 8000 - Loss = 25.468841552734375
Iteration 2000 / 8000 - Loss = 25.641395568847656
Iteration 3000 / 8000 - Loss = 26.226329803466797
Iteration 4000 / 8000 - Loss = 26.115421295166016
Iteration 5000 / 8000 - Loss = 26.033262252807617
Iteration 6000 / 8000 - Loss = 26.249740600585938
Iteration 7000 / 8000 - Loss = 26.376014709472656
Average train epoch loss for epoch 45 = 26.073788166046143
Epoch 45 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 45 - Vscore = 0.7028234025612794, ARI = 0.48922173210999187, AMI = 0.6528471949756367
Iteration 1000 / 8000 - Loss = 25.9993839263916
Iteration 2000 / 8000 - Loss = 25.825834274291992
Iteration 3000 / 8000 - Loss = 25.996402740478516
Iteration 4000 / 8000 - Loss = 26.327505111694336
Iteration 5000 / 8000 - Loss = 26.139497756958008
Iteration 6000 / 8000 - Loss = 26.056278228759766
Iteration 7000 / 8000 - Loss = 26.198530197143555
Average train epoch loss for epoch 46 = 26.053321599960327
Epoch 46 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 46 - Vscore = 0.702959361592522, ARI = 0.49013789170210414, AMI = 0.6536635530024318
Iteration 1000 / 8000 - Loss = 25.8388614654541
Iteration 2000 / 8000 - Loss = 25.628252029418945
Iteration 3000 / 8000 - Loss = 26.20027732849121
Iteration 4000 / 8000 - Loss = 26.294710159301758
Iteration 5000 / 8000 - Loss = 26.471778869628906
Iteration 6000 / 8000 - Loss = 26.00231170654297
Iteration 7000 / 8000 - Loss = 26.94290542602539
Average train epoch loss for epoch 47 = 26.03940987586975
Epoch 47 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 47 - Vscore = 0.7013507387994595, ARI = 0.48699030786054637, AMI = 0.6525689078716614
Iteration 1000 / 8000 - Loss = 26.58810806274414
Iteration 2000 / 8000 - Loss = 26.45677947998047
Iteration 3000 / 8000 - Loss = 25.993528366088867
Iteration 4000 / 8000 - Loss = 25.984529495239258
Iteration 5000 / 8000 - Loss = 25.691083908081055
Iteration 6000 / 8000 - Loss = 26.620899200439453
Iteration 7000 / 8000 - Loss = 25.78738784790039
Average train epoch loss for epoch 48 = 26.025039076805115
Epoch 48 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 48 - Vscore = 0.6990475066432817, ARI = 0.4889807556109711, AMI = 0.6492859350414835
Iteration 1000 / 8000 - Loss = 26.486919403076172
Iteration 2000 / 8000 - Loss = 26.407148361206055
Iteration 3000 / 8000 - Loss = 26.398845672607422
Iteration 4000 / 8000 - Loss = 25.512948989868164
Iteration 5000 / 8000 - Loss = 25.91913604736328
Iteration 6000 / 8000 - Loss = 26.475696563720703
Iteration 7000 / 8000 - Loss = 25.72054672241211
Average train epoch loss for epoch 49 = 26.03343951702118
Epoch 49 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 49 - Vscore = 0.7000813466278399, ARI = 0.48535526358476444, AMI = 0.6498053943183177
Iteration 1000 / 8000 - Loss = 26.026121139526367
Iteration 2000 / 8000 - Loss = 26.26468849182129
Iteration 3000 / 8000 - Loss = 25.93936538696289
Iteration 4000 / 8000 - Loss = 25.270252227783203
Iteration 5000 / 8000 - Loss = 25.820863723754883
Iteration 6000 / 8000 - Loss = 26.02340316772461
Iteration 7000 / 8000 - Loss = 26.051084518432617
Average train epoch loss for epoch 50 = 25.982879042625427
Epoch 50 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 50 - Vscore = 0.6988038961499927, ARI = 0.48692898974161836, AMI = 0.6485525251162677
Iteration 1000 / 8000 - Loss = 25.789897918701172
Iteration 2000 / 8000 - Loss = 26.276086807250977
Iteration 3000 / 8000 - Loss = 25.816625595092773
Iteration 4000 / 8000 - Loss = 25.6816463470459
Iteration 5000 / 8000 - Loss = 25.439754486083984
Iteration 6000 / 8000 - Loss = 25.892986297607422
Iteration 7000 / 8000 - Loss = 26.169492721557617
Average train epoch loss for epoch 51 = 25.903843998908997
Epoch 51 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 51 - Vscore = 0.7013952558245503, ARI = 0.48891793901650615, AMI = 0.6511675722328387
Iteration 1000 / 8000 - Loss = 25.710758209228516
Iteration 2000 / 8000 - Loss = 25.826820373535156
Iteration 3000 / 8000 - Loss = 26.034547805786133
Iteration 4000 / 8000 - Loss = 25.68270492553711
Iteration 5000 / 8000 - Loss = 25.76612663269043
Iteration 6000 / 8000 - Loss = 26.675251007080078
Iteration 7000 / 8000 - Loss = 26.309192657470703
Average train epoch loss for epoch 52 = 25.884446024894714
Epoch 52 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 52 - Vscore = 0.7015543687425597, ARI = 0.48756183514716483, AMI = 0.6508040901090881
Iteration 1000 / 8000 - Loss = 25.773475646972656
Iteration 2000 / 8000 - Loss = 25.921823501586914
Iteration 3000 / 8000 - Loss = 26.20945930480957
Iteration 4000 / 8000 - Loss = 26.061044692993164
Iteration 5000 / 8000 - Loss = 25.211505889892578
Iteration 6000 / 8000 - Loss = 26.27806854248047
Iteration 7000 / 8000 - Loss = 25.343048095703125
Average train epoch loss for epoch 53 = 25.871296763420105
Epoch 53 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 53 - Vscore = 0.6998964027239738, ARI = 0.4868401127742322, AMI = 0.6490593638266408
Iteration 1000 / 8000 - Loss = 25.810117721557617
Iteration 2000 / 8000 - Loss = 26.063650131225586
Iteration 3000 / 8000 - Loss = 25.501516342163086
Iteration 4000 / 8000 - Loss = 25.842845916748047
Iteration 5000 / 8000 - Loss = 25.481857299804688
Iteration 6000 / 8000 - Loss = 25.513839721679688
Iteration 7000 / 8000 - Loss = 26.247480392456055
Average train epoch loss for epoch 54 = 25.86115860939026
Epoch 54 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 54 - Vscore = 0.6990849589989692, ARI = 0.48429449852462647, AMI = 0.6479103764896761
Iteration 1000 / 8000 - Loss = 25.640796661376953
Iteration 2000 / 8000 - Loss = 25.831735610961914
Iteration 3000 / 8000 - Loss = 25.72803497314453
Iteration 4000 / 8000 - Loss = 26.41616439819336
Iteration 5000 / 8000 - Loss = 25.51107406616211
Iteration 6000 / 8000 - Loss = 25.97627830505371
Iteration 7000 / 8000 - Loss = 26.6343994140625
Average train epoch loss for epoch 55 = 25.855695366859436
Epoch 55 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 55 - Vscore = 0.6992986655810247, ARI = 0.4859152260833747, AMI = 0.6485294834856703
Iteration 1000 / 8000 - Loss = 26.2088565826416
Iteration 2000 / 8000 - Loss = 26.429603576660156
Iteration 3000 / 8000 - Loss = 25.924579620361328
Iteration 4000 / 8000 - Loss = 25.810962677001953
Iteration 5000 / 8000 - Loss = 26.02525520324707
Iteration 6000 / 8000 - Loss = 26.01782989501953
Iteration 7000 / 8000 - Loss = 25.804529190063477
Average train epoch loss for epoch 56 = 25.845237016677856
Epoch 56 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 56 - Vscore = 0.7000229855996032, ARI = 0.4859798534278196, AMI = 0.6487038527318776
Iteration 1000 / 8000 - Loss = 26.179275512695312
Iteration 2000 / 8000 - Loss = 25.971834182739258
Iteration 3000 / 8000 - Loss = 25.601722717285156
Iteration 4000 / 8000 - Loss = 25.56143569946289
Iteration 5000 / 8000 - Loss = 25.757177352905273
Iteration 6000 / 8000 - Loss = 26.289430618286133
Iteration 7000 / 8000 - Loss = 25.930299758911133
Average train epoch loss for epoch 57 = 25.83485972881317
Epoch 57 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 57 - Vscore = 0.698211945932567, ARI = 0.4840710883699088, AMI = 0.6467201392004317
Iteration 1000 / 8000 - Loss = 26.158056259155273
Iteration 2000 / 8000 - Loss = 25.284990310668945
Iteration 3000 / 8000 - Loss = 25.621700286865234
Iteration 4000 / 8000 - Loss = 25.55702781677246
Iteration 5000 / 8000 - Loss = 25.708919525146484
Iteration 6000 / 8000 - Loss = 26.31340789794922
Iteration 7000 / 8000 - Loss = 26.19978141784668
Average train epoch loss for epoch 58 = 25.830554842948914
Epoch 58 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 58 - Vscore = 0.7019376059820899, ARI = 0.4873338188203583, AMI = 0.6509645748508799
Iteration 1000 / 8000 - Loss = 25.77143096923828
Iteration 2000 / 8000 - Loss = 25.70189094543457
Iteration 3000 / 8000 - Loss = 26.060956954956055
Iteration 4000 / 8000 - Loss = 25.760507583618164
Iteration 5000 / 8000 - Loss = 26.849618911743164
Iteration 6000 / 8000 - Loss = 25.811681747436523
Iteration 7000 / 8000 - Loss = 25.928266525268555
Average train epoch loss for epoch 59 = 25.828616857528687
Epoch 59 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 59 - Vscore = 0.6998609070040315, ARI = 0.4843580020223667, AMI = 0.6481897872321076
Iteration 1000 / 8000 - Loss = 25.895414352416992
Iteration 2000 / 8000 - Loss = 25.607481002807617
Iteration 3000 / 8000 - Loss = 25.91773223876953
Iteration 4000 / 8000 - Loss = 25.470691680908203
Iteration 5000 / 8000 - Loss = 25.90378761291504
Iteration 6000 / 8000 - Loss = 25.86907196044922
Iteration 7000 / 8000 - Loss = 25.66468048095703
Average train epoch loss for epoch 60 = 25.82037103176117
Epoch 60 - Eval mode
Test label shape = (2000,), Predicted label shape = (2000,)
Epoch 60 - Vscore = 0.7003382005202555, ARI = 0.48586750377542115, AMI = 0.6486443332617027
Training finished
Epoch level Train_Loss, Test_Loss, V_Score, ARI, AMI -:
[44.61981773376465, 42.83835220336914, 41.14787316322327, 39.5243284702301, 38.040292263031006, 36.671791791915894, 35.50427722930908, 34.46940732002258, 33.529078006744385, 32.65729594230652, 31.868276357650757, 31.183314085006714, 30.56778347492218, 30.002180337905884, 29.486168146133423, 29.052592754364014, 28.653364658355713, 28.310531497001648, 28.032676577568054, 27.759687662124634, 27.49682593345642, 27.298112511634827, 27.14396059513092, 26.992637872695923, 26.875083446502686, 26.65231204032898, 26.562967538833618, 26.516089916229248, 26.469815135002136, 26.41976499557495, 26.385303735733032, 26.350269198417664, 26.31752061843872, 26.286338090896606, 26.264025330543518, 26.238107681274414, 26.216638922691345, 26.1919105052948, 26.182292222976685, 26.14860248565674, 26.13693141937256, 26.116631269454956, 26.100979328155518, 26.093749046325684, 26.073788166046143, 26.053321599960327, 26.03940987586975, 26.025039076805115, 26.03343951702118, 25.982879042625427, 25.903843998908997, 25.884446024894714, 25.871296763420105, 25.86115860939026, 25.855695366859436, 25.845237016677856, 25.83485972881317, 25.830554842948914, 25.828616857528687, 25.82037103176117]
[43.66981220245361, 42.07459831237793, 40.38055419921875, 38.84724140167236, 37.372514724731445, 36.10756015777588, 35.03053379058838, 34.06310844421387, 33.17483711242676, 32.333909034729004, 31.611019134521484, 30.970089435577393, 30.38783597946167, 29.83281373977661, 29.398672103881836, 28.964163780212402, 28.600244522094727, 28.28859519958496, 28.034929752349854, 27.78718137741089, 27.569053649902344, 27.39863395690918, 27.266531467437744, 27.161646842956543, 27.03472137451172, 26.893814086914062, 26.82444477081299, 26.78041362762451, 26.734761238098145, 26.720706939697266, 26.66789150238037, 26.631797313690186, 26.623374462127686, 26.592753887176514, 26.57185125350952, 26.564467430114746, 26.52099895477295, 26.54692554473877, 26.50927448272705, 26.495752334594727, 26.45380401611328, 26.46595859527588, 26.445876598358154, 26.42610502243042, 26.421220302581787, 26.391191482543945, 26.37597894668579, 26.41134548187256, 26.36579704284668, 26.375725746154785, 26.31986904144287, 26.30302381515503, 26.300330638885498, 26.297982692718506, 26.284281730651855, 26.293559074401855, 26.28928852081299, 26.284932613372803, 26.290526390075684, 26.28827667236328]
[0.3618294868423858, 0.4714971198337818, 0.500078155070635, 0.5342629081957665, 0.5866289512787091, 0.6197420973201849, 0.6517846811882854, 0.6733365189466051, 0.6798800343593567, 0.6844441911183624, 0.6908909188532137, 0.694371843442731, 0.6946071489381981, 0.6958150907666806, 0.7026769710027627, 0.7036022050729589, 0.7034057799078066, 0.7073034587139532, 0.7090947357567222, 0.7027200241452135, 0.7033528221313655, 0.702606774088673, 0.7032359072214717, 0.7046663641731108, 0.700744177016709, 0.7021980172511908, 0.7034279270422903, 0.7011030479723017, 0.7001491904180508, 0.7049663406455517, 0.7035996579255634, 0.7049629512828006, 0.7041796652564563, 0.705742154558113, 0.7080280688912498, 0.7059668125420938, 0.7049312238309631, 0.7058227614333411, 0.7069397813995092, 0.7035604963262062, 0.7051454403957585, 0.7043378570657856, 0.7025939196658003, 0.7048822482260255, 0.7028234025612794, 0.702959361592522, 0.7013507387994595, 0.6990475066432817, 0.7000813466278399, 0.6988038961499927, 0.7013952558245503, 0.7015543687425597, 0.6998964027239738, 0.6990849589989692, 0.6992986655810247, 0.7000229855996032, 0.698211945932567, 0.7019376059820899, 0.6998609070040315, 0.7003382005202555]
[0.21674685317346645, 0.3364090736875596, 0.3498781270531325, 0.37798738131479875, 0.42667275615541883, 0.47004525988476364, 0.5023825447339653, 0.5175051544535477, 0.5200278903898246, 0.5141807925890026, 0.508874212532295, 0.5073234698994905, 0.49639150066301496, 0.4942241931746379, 0.4956276352502448, 0.4946983670841489, 0.49433127185537756, 0.4980553587490815, 0.5009104616789678, 0.4980491592818407, 0.4960279803695325, 0.49655374758673754, 0.4957810381300904, 0.49435895058851326, 0.4912618989151982, 0.4926434808213699, 0.4906261920041116, 0.48701603512673614, 0.48729425263896664, 0.49098039400856464, 0.48812391914702086, 0.48808376650023955, 0.4886435622767034, 0.49009452113099033, 0.49297554703534724, 0.4923504556890125, 0.4891101471526743, 0.49241796254716796, 0.4931878023878558, 0.4909154917878728, 0.490911879429626, 0.4903653465058634, 0.4897631619694284, 0.49045563237068296, 0.48922173210999187, 0.49013789170210414, 0.48699030786054637, 0.4889807556109711, 0.48535526358476444, 0.48692898974161836, 0.48891793901650615, 0.48756183514716483, 0.4868401127742322, 0.48429449852462647, 0.4859152260833747, 0.4859798534278196, 0.4840710883699088, 0.4873338188203583, 0.4843580020223667, 0.48586750377542115]
[0.3411988390377493, 0.433606745951033, 0.4559990288869092, 0.4902277678680698, 0.5424568996253448, 0.5766506509995158, 0.6087214731766533, 0.63089544149184, 0.6390594861294816, 0.6435290823878768, 0.6503432957099283, 0.6539662510474074, 0.6542555797493497, 0.6556367300147984, 0.6628236846995792, 0.6636454936098857, 0.6629679443073905, 0.6663532205548528, 0.667702258199486, 0.6600487747437122, 0.660352882684789, 0.6587871731678517, 0.6589893560819308, 0.6595216781462884, 0.6547579166983216, 0.6566668274703067, 0.6569253489532058, 0.6546323210699019, 0.6536646737767187, 0.657805699819362, 0.6560401246886695, 0.6575429857686182, 0.6568600643865556, 0.6579420996091335, 0.6607181720902334, 0.6579768759117404, 0.6571791711813519, 0.6579214105570415, 0.6591875047415608, 0.6553264604108198, 0.6564782032733119, 0.6557595871287303, 0.6541288630073397, 0.6564465685792996, 0.6528471949756367, 0.6536635530024318, 0.6525689078716614, 0.6492859350414835, 0.6498053943183177, 0.6485525251162677, 0.6511675722328387, 0.6508040901090881, 0.6490593638266408, 0.6479103764896761, 0.6485294834856703, 0.6487038527318776, 0.6467201392004317, 0.6509645748508799, 0.6481897872321076, 0.6486443332617027]
Performing k-means clustering to 14 components of 8000 samples in dimension 32/32 ...
K-means took: 0.9901607036590576 sec
KMeans V_Score, ARI, AMI -: 0.2874743447551756, 0.16684882860045652, 0.2675854168434067
