Running --> python3 /nas/longleaf/home/athreya/gmm/scripts/train.py --cfg_file=params.json
{'att_dims': 128,
 'batch_size': 3000,
 'clip': 0.01,
 'data_root': '/nas/longleaf/home/athreya/gmm/data/',
 'epochs': 30,
 'exp_dir': '/nas/longleaf/home/athreya/gmm/model/',
 'k': 14,
 'lr': 0.01,
 'resume_from_epoch': -1,
 'seed': 1234,
 'tau': 1,
 'weight_decay': 0}
Device -: cuda:0
Starting training
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [44.063941955566406, 229.31500244140625, 44.063941955566406]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [43.02372360229492, 249.60421752929688, 43.02372360229492]
Average train epoch loss for epoch 1 = 43.8690516608102
Epoch 1 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 1 - Test Loss = 42.22482681274414, Vscore = 0.4492740154295921, ARI = 0.2811046996493927, AMI = 0.4124109366087196
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [41.206748962402344, 242.5519561767578, 43.632266998291016]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [40.329349517822266, 196.01260375976562, 42.28947448730469]
Average train epoch loss for epoch 2 = 40.90412521362305
Epoch 2 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 2 - Test Loss = 39.58830370221819, Vscore = 0.6187750640383248, ARI = 0.48426464301786704, AMI = 0.5740220288405536
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [38.83847427368164, 126.79015350341797, 40.10637664794922]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [37.91898727416992, 113.92050170898438, 39.05819320678711]
Average train epoch loss for epoch 3 = 38.40088667188372
Epoch 3 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 3 - Test Loss = 37.25420870099749, Vscore = 0.6792855954186886, ARI = 0.5895221657411484, AMI = 0.6472235093958999
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [36.437469482421875, 106.76689910888672, 37.5051383972168]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [35.81674575805664, 105.42234802246094, 36.870967864990234]
Average train epoch loss for epoch 4 = 36.23063128335135
Epoch 4 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 4 - Test Loss = 35.24067905970982, Vscore = 0.7501303796705562, ARI = 0.7173389344748189, AMI = 0.7364553090163175
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [34.42909240722656, 103.4044189453125, 35.463134765625]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [33.99142074584961, 101.07752227783203, 35.002197265625]
Average train epoch loss for epoch 5 = 34.352445874895366
Epoch 5 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 5 - Test Loss = 33.5677375793457, Vscore = 0.7817856668262629, ARI = 0.766413603835417, AMI = 0.7746452480442974
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [33.095909118652344, 98.4142837524414, 34.08005142211914]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [32.286197662353516, 97.96646118164062, 33.26586151123047]
Average train epoch loss for epoch 6 = 32.84619127001081
Epoch 6 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 6 - Test Loss = 32.233045850481304, Vscore = 0.7853445805340348, ARI = 0.7771984370738988, AMI = 0.7767982504702243
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [31.561307907104492, 98.01213073730469, 32.54142761230469]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [31.457286834716797, 98.07765197753906, 32.43806457519531]
Average train epoch loss for epoch 7 = 31.632391793387278
Epoch 7 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 7 - Test Loss = 31.16210447038923, Vscore = 0.7901041480132583, ARI = 0.7771851154212728, AMI = 0.7795569014526412
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [30.897083282470703, 96.13662719726562, 31.858449935913086]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [30.392879486083984, 95.47801208496094, 31.347660064697266]
Average train epoch loss for epoch 8 = 30.668476036616735
Epoch 8 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 8 - Test Loss = 30.357593263898575, Vscore = 0.7992539993290301, ARI = 0.7741052970946414, AMI = 0.7902992800747343
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [29.799678802490234, 94.78256225585938, 30.74750518798828]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [29.727596282958984, 93.64351654052734, 30.664031982421875]
Average train epoch loss for epoch 9 = 29.9293030330113
Epoch 9 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 9 - Test Loss = 29.7082029070173, Vscore = 0.795533044224979, ARI = 0.7604717389882986, AMI = 0.7865881969932791
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [29.812238693237305, 91.29683685302734, 30.72520637512207]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [29.486936569213867, 90.20734405517578, 30.389009475708008]
Average train epoch loss for epoch 10 = 29.41782603945051
Epoch 10 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 10 - Test Loss = 29.308415821620397, Vscore = 0.7843583253531718, ARI = 0.7327103534444955, AMI = 0.7728884648521785
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [29.030338287353516, 89.26463317871094, 29.922985076904297]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.684885025024414, 88.69474029541016, 29.57183265686035]
Average train epoch loss for epoch 11 = 29.084948812212264
Epoch 11 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 11 - Test Loss = 29.026874814714706, Vscore = 0.7789639194964322, ARI = 0.7068514618388236, AMI = 0.7695942124973645
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.91151237487793, 86.4928207397461, 29.776439666748047]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [29.14410972595215, 86.33125305175781, 30.007421493530273]
Average train epoch loss for epoch 12 = 28.845974104745046
Epoch 12 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 12 - Test Loss = 28.822702407836914, Vscore = 0.7777062932093194, ARI = 0.6891834860665449, AMI = 0.7700928904196287
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.825849533081055, 86.71066284179688, 29.692956924438477]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.528804779052734, 87.10212707519531, 29.399826049804688]
Average train epoch loss for epoch 13 = 28.655914579119003
Epoch 13 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 13 - Test Loss = 28.665301731654576, Vscore = 0.7768100948122484, ARI = 0.681027207888209, AMI = 0.771931115101075
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.60126495361328, 86.21405029296875, 29.46340560913086]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.493934631347656, 86.59927368164062, 29.359928131103516]
Average train epoch loss for epoch 14 = 28.50446993964059
Epoch 14 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 14 - Test Loss = 28.515425818307058, Vscore = 0.7768977960859106, ARI = 0.6790790372393372, AMI = 0.775269441185923
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.584022521972656, 85.9487533569336, 29.443510055541992]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.47797966003418, 85.53886413574219, 29.3333683013916]
Average train epoch loss for epoch 15 = 28.379661219460623
Epoch 15 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 15 - Test Loss = 28.466105597359793, Vscore = 0.78150554394867, ARI = 0.6860257396119791, AMI = 0.7773481555400852
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.395849227905273, 85.69970703125, 29.252845764160156]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [27.951915740966797, 85.23484802246094, 28.804264068603516]
Average train epoch loss for epoch 16 = 28.28259434018816
Epoch 16 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 16 - Test Loss = 28.38445908682687, Vscore = 0.7840129656745025, ARI = 0.6915353209458208, AMI = 0.7768785959407569
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.222549438476562, 85.20042419433594, 29.074554443359375]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [27.930078506469727, 85.19081115722656, 28.781986236572266]
Average train epoch loss for epoch 17 = 28.226988656180247
Epoch 17 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 17 - Test Loss = 28.319193976266043, Vscore = 0.7867270728286855, ARI = 0.6972514119715223, AMI = 0.7768872766954872
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.278228759765625, 85.16654968261719, 29.129894256591797]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.3958797454834, 85.11769104003906, 29.24705696105957]
Average train epoch loss for epoch 18 = 28.201763766152517
Epoch 18 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 18 - Test Loss = 28.334584644862584, Vscore = 0.7875942769189386, ARI = 0.6998920961031228, AMI = 0.776049821675323
Test loss 28.334584644862584 not better than previous best test loss 28.319193976266043. Skipping saving model
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.060020446777344, 84.1617431640625, 28.90163803100586]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.003087997436523, 83.71576690673828, 28.840246200561523]
Average train epoch loss for epoch 19 = 28.17257070541382
Epoch 19 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 19 - Test Loss = 28.286743436540878, Vscore = 0.7872938364869755, ARI = 0.7009508268585625, AMI = 0.7747527295375882
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.25818634033203, 83.1689453125, 29.089876174926758]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.502796173095703, 83.16238403320312, 29.33441925048828]
Average train epoch loss for epoch 20 = 28.16314915248326
Epoch 20 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 20 - Test Loss = 28.324125289916992, Vscore = 0.7887416624562195, ARI = 0.7045285259733735, AMI = 0.7749009565425179
Test loss 28.324125289916992 not better than previous best test loss 28.286743436540878. Skipping saving model
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.077890396118164, 83.1584243774414, 28.909475326538086]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [27.749692916870117, 83.15713500976562, 28.58126449584961]
Average train epoch loss for epoch 21 = 28.099784782954625
Epoch 21 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 21 - Test Loss = 28.21653802054269, Vscore = 0.7890904824891417, ARI = 0.7035387316252395, AMI = 0.7756764862356905
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [27.920692443847656, 83.15568542480469, 28.752248764038086]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [27.889432907104492, 83.15596008300781, 28.720993041992188]
Average train epoch loss for epoch 22 = 28.075324603489467
Epoch 22 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 22 - Test Loss = 28.202695846557617, Vscore = 0.7892188571689203, ARI = 0.7049588564380888, AMI = 0.7747181151831235
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.161012649536133, 83.15628051757812, 28.99257469177246]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.10341453552246, 83.15676879882812, 28.934982299804688]
Average train epoch loss for epoch 23 = 28.064509391784668
Epoch 23 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 23 - Test Loss = 28.2000675201416, Vscore = 0.7865461311555338, ARI = 0.7012674650086298, AMI = 0.7734739902721857
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [27.831663131713867, 83.15794372558594, 28.66324234008789]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.098560333251953, 83.16688537597656, 28.93022918701172]
Average train epoch loss for epoch 24 = 28.05921827043806
Epoch 24 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 24 - Test Loss = 28.19504383632115, Vscore = 0.7863828331760366, ARI = 0.7012097614915298, AMI = 0.7731076472724002
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [27.994375228881836, 83.18386840820312, 28.826213836669922]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [27.931379318237305, 83.19076538085938, 28.763286590576172]
Average train epoch loss for epoch 25 = 28.05143860408238
Epoch 25 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 25 - Test Loss = 28.18517575945173, Vscore = 0.7857953438431112, ARI = 0.6988665608047309, AMI = 0.7729877825113844
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [27.78411102294922, 83.25763702392578, 28.616687774658203]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.233924865722656, 83.28175354003906, 29.066741943359375]
Average train epoch loss for epoch 26 = 28.04632180077689
Epoch 26 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 26 - Test Loss = 28.194929395403182, Vscore = 0.7857403181548042, ARI = 0.6983171656041504, AMI = 0.7728063875002009
Test loss 28.194929395403182 not better than previous best test loss 28.18517575945173. Skipping saving model
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.140039443969727, 83.67426300048828, 28.976781845092773]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.006389617919922, 83.96678924560547, 28.846057891845703]
Average train epoch loss for epoch 27 = 28.028544085366384
Epoch 27 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 27 - Test Loss = 28.16160719735282, Vscore = 0.7833683263429012, ARI = 0.6941712505658507, AMI = 0.7712185680725773
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.249271392822266, 84.06494903564453, 29.089920043945312]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.075044631958008, 84.07878112792969, 28.91583251953125]
Average train epoch loss for epoch 28 = 28.010560989379883
Epoch 28 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 28 - Test Loss = 28.167711530412948, Vscore = 0.7825117468559886, ARI = 0.6914399016400183, AMI = 0.7704617398435416
Test loss 28.167711530412948 not better than previous best test loss 28.16160719735282. Skipping saving model
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [27.763965606689453, 84.0903549194336, 28.604869842529297]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [27.993770599365234, 84.09306335449219, 28.834701538085938]
Average train epoch loss for epoch 29 = 28.009800502232142
Epoch 29 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 29 - Test Loss = 28.15352984837123, Vscore = 0.7819436011952476, ARI = 0.6875007226225773, AMI = 0.7699844563898574
Iteration 30000 / 83347 - Loss (nll, l1_loss, total) = [28.055665969848633, 84.09580993652344, 28.896623611450195]
Iteration 60000 / 83347 - Loss (nll, l1_loss, total) = [28.04949951171875, 84.09684753417969, 28.89046859741211]
Average train epoch loss for epoch 30 = 28.008275713239396
Epoch 30 - Eval mode
Test label shape = (20837,), Predicted label shape = (20837,)
Epoch 30 - Test Loss = 28.149641581944056, Vscore = 0.7790767030765513, ARI = 0.6819595349326444, AMI = 0.7681539669308172
Training finished
Epoch level Train_Loss, Test_Loss, V_Score, ARI, AMI -:
[43.8690516608102, 40.90412521362305, 38.40088667188372, 36.23063128335135, 34.352445874895366, 32.84619127001081, 31.632391793387278, 30.668476036616735, 29.9293030330113, 29.41782603945051, 29.084948812212264, 28.845974104745046, 28.655914579119003, 28.50446993964059, 28.379661219460623, 28.28259434018816, 28.226988656180247, 28.201763766152517, 28.17257070541382, 28.16314915248326, 28.099784782954625, 28.075324603489467, 28.064509391784668, 28.05921827043806, 28.05143860408238, 28.04632180077689, 28.028544085366384, 28.010560989379883, 28.009800502232142, 28.008275713239396]
[42.22482681274414, 39.58830370221819, 37.25420870099749, 35.24067905970982, 33.5677375793457, 32.233045850481304, 31.16210447038923, 30.357593263898575, 29.7082029070173, 29.308415821620397, 29.026874814714706, 28.822702407836914, 28.665301731654576, 28.515425818307058, 28.466105597359793, 28.38445908682687, 28.319193976266043, 28.334584644862584, 28.286743436540878, 28.324125289916992, 28.21653802054269, 28.202695846557617, 28.2000675201416, 28.19504383632115, 28.18517575945173, 28.194929395403182, 28.16160719735282, 28.167711530412948, 28.15352984837123, 28.149641581944056]
[0.4492740154295921, 0.6187750640383248, 0.6792855954186886, 0.7501303796705562, 0.7817856668262629, 0.7853445805340348, 0.7901041480132583, 0.7992539993290301, 0.795533044224979, 0.7843583253531718, 0.7789639194964322, 0.7777062932093194, 0.7768100948122484, 0.7768977960859106, 0.78150554394867, 0.7840129656745025, 0.7867270728286855, 0.7875942769189386, 0.7872938364869755, 0.7887416624562195, 0.7890904824891417, 0.7892188571689203, 0.7865461311555338, 0.7863828331760366, 0.7857953438431112, 0.7857403181548042, 0.7833683263429012, 0.7825117468559886, 0.7819436011952476, 0.7790767030765513]
[0.2811046996493927, 0.48426464301786704, 0.5895221657411484, 0.7173389344748189, 0.766413603835417, 0.7771984370738988, 0.7771851154212728, 0.7741052970946414, 0.7604717389882986, 0.7327103534444955, 0.7068514618388236, 0.6891834860665449, 0.681027207888209, 0.6790790372393372, 0.6860257396119791, 0.6915353209458208, 0.6972514119715223, 0.6998920961031228, 0.7009508268585625, 0.7045285259733735, 0.7035387316252395, 0.7049588564380888, 0.7012674650086298, 0.7012097614915298, 0.6988665608047309, 0.6983171656041504, 0.6941712505658507, 0.6914399016400183, 0.6875007226225773, 0.6819595349326444]
[0.4124109366087196, 0.5740220288405536, 0.6472235093958999, 0.7364553090163175, 0.7746452480442974, 0.7767982504702243, 0.7795569014526412, 0.7902992800747343, 0.7865881969932791, 0.7728884648521785, 0.7695942124973645, 0.7700928904196287, 0.771931115101075, 0.775269441185923, 0.7773481555400852, 0.7768785959407569, 0.7768872766954872, 0.776049821675323, 0.7747527295375882, 0.7749009565425179, 0.7756764862356905, 0.7747181151831235, 0.7734739902721857, 0.7731076472724002, 0.7729877825113844, 0.7728063875002009, 0.7712185680725773, 0.7704617398435416, 0.7699844563898574, 0.7681539669308172]
Performing k-means clustering to 14 components of 83347 samples in dimension 32/32 ...
K-means took: 7.789522171020508 sec
KMeans V_Score, ARI, AMI -: 0.36736081096457945, 0.3096971673359886, 0.34759860530272074

Model alpha

tensor([[0.3126, 0.3272, 0.2974, 0.2858, 0.2973, 0.3196, 0.3208, 0.3260, 0.3252,
         0.2869, 0.3181, 0.3262, 0.2922, 0.2821, 0.3132, 0.3234, 0.3233, 0.2842,
         0.3191, 0.3213, 0.3254, 0.3248, 0.3190, 0.2866, 0.3228, 0.2857, 0.3240,
         0.3252, 0.2913, 0.2914, 0.3193, 0.2875],
        [0.7443, 0.6305, 0.2477, 0.2685, 0.7898, 0.7391, 0.7324, 0.7192, 0.2988,
         0.2568, 0.6983, 0.3171, 0.2545, 0.2488, 0.7366, 0.7356, 0.7831, 0.2410,
         0.7903, 0.2510, 0.7622, 0.2763, 0.7393, 0.2393, 0.7586, 0.2340, 0.7029,
         0.2842, 0.7290, 0.2907, 0.7805, 0.6984],
        [0.3185, 0.2937, 0.3638, 0.3320, 0.3377, 0.2813, 0.3303, 0.3335, 0.7060,
         0.2954, 0.3250, 0.3293, 0.3269, 0.2777, 0.3176, 0.3339, 0.2869, 0.2836,
         0.3612, 0.3895, 0.3314, 0.3591, 0.3259, 0.3219, 0.3174, 0.3263, 0.3271,
         0.6696, 0.2892, 0.3216, 0.6085, 0.2920],
        [0.6702, 0.3311, 0.2678, 0.2618, 0.3021, 0.3090, 0.6837, 0.3403, 0.3176,
         0.2599, 0.6705, 0.3098, 0.6939, 0.7124, 0.3209, 0.3172, 0.3329, 0.2781,
         0.3041, 0.2973, 0.3223, 0.3103, 0.3247, 0.2769, 0.6702, 0.3003, 0.2849,
         0.2899, 0.2863, 0.7001, 0.2920, 0.2923],
        [0.7391, 0.2783, 0.2272, 0.2540, 0.2613, 0.7553, 0.7134, 0.7189, 0.2902,
         0.2542, 0.7087, 0.3272, 0.2424, 0.2297, 0.2644, 0.2954, 0.2659, 0.2590,
         0.2605, 0.7990, 0.3152, 0.7549, 0.7372, 0.2605, 0.2802, 0.2607, 0.3059,
         0.2953, 0.2537, 0.7793, 0.2249, 0.2570],
        [0.7391, 0.2998, 0.2753, 0.2934, 0.2893, 0.7390, 0.2827, 0.7641, 0.3188,
         0.2906, 0.3059, 0.3215, 0.7263, 0.2979, 0.7563, 0.3166, 0.6962, 0.2694,
         0.2876, 0.2893, 0.3308, 0.3051, 0.3130, 0.2715, 0.6889, 0.2526, 0.3190,
         0.7407, 0.2936, 0.3350, 0.2871, 0.2787],
        [0.7371, 0.6957, 0.2422, 0.2591, 0.2598, 0.7505, 0.8001, 0.7086, 0.3028,
         0.2634, 0.3080, 0.2975, 0.2489, 0.2370, 0.2747, 0.2912, 0.2363, 0.2503,
         0.2463, 0.8019, 0.3156, 0.2350, 0.2853, 0.2612, 0.2772, 0.7601, 0.3165,
         0.3192, 0.2638, 0.2765, 0.2333, 0.2560],
        [0.3219, 0.3246, 0.3076, 0.3146, 0.3260, 0.3140, 0.3235, 0.3286, 0.3259,
         0.6357, 0.3226, 0.2985, 0.2937, 0.2839, 0.3203, 0.3239, 0.3245, 0.2824,
         0.3203, 0.3025, 0.3257, 0.3258, 0.3193, 0.2898, 0.3242, 0.2875, 0.3265,
         0.3282, 0.2974, 0.2935, 0.3184, 0.2812],
        [0.3201, 0.2968, 0.2888, 0.2827, 0.3102, 0.2864, 0.3188, 0.7258, 0.3256,
         0.2797, 0.3234, 0.2964, 0.6929, 0.2800, 0.3234, 0.3253, 0.2847, 0.2843,
         0.3162, 0.3010, 0.3273, 0.2835, 0.3218, 0.2869, 0.3263, 0.2898, 0.3259,
         0.3218, 0.2867, 0.2953, 0.3179, 0.3214],
        [0.3263, 0.3216, 0.2991, 0.2965, 0.3166, 0.3220, 0.3244, 0.3321, 0.3235,
         0.3182, 0.3156, 0.3248, 0.2952, 0.2851, 0.3186, 0.3182, 0.3197, 0.2908,
         0.7394, 0.3150, 0.3249, 0.3197, 0.3082, 0.2879, 0.3185, 0.3106, 0.3189,
         0.3260, 0.2833, 0.3184, 0.6993, 0.2908],
        [0.7424, 0.2411, 0.7914, 0.8055, 0.2576, 0.7617, 0.7592, 0.2862, 0.3114,
         0.8156, 0.3232, 0.3181, 0.2377, 0.2357, 0.3043, 0.2972, 0.2323, 0.2459,
         0.2541, 0.2492, 0.3044, 0.2739, 0.3023, 0.2547, 0.2907, 0.2597, 0.3153,
         0.7007, 0.3201, 0.2954, 0.2670, 0.2848],
        [0.7063, 0.7136, 0.2667, 0.2650, 0.2850, 0.3073, 0.7537, 0.3269, 0.3211,
         0.2616, 0.3083, 0.3237, 0.2679, 0.2661, 0.3017, 0.3159, 0.2603, 0.2734,
         0.2616, 0.2957, 0.2892, 0.3086, 0.3034, 0.2750, 0.3071, 0.7271, 0.3109,
         0.3221, 0.2799, 0.2975, 0.2911, 0.2760],
        [0.7391, 0.7585, 0.2342, 0.2600, 0.2533, 0.7488, 0.7408, 0.7142, 0.2973,
         0.2500, 0.3045, 0.3135, 0.2493, 0.2439, 0.2888, 0.2916, 0.2781, 0.2540,
         0.7100, 0.2789, 0.2610, 0.2831, 0.7082, 0.7309, 0.2930, 0.2521, 0.3142,
         0.3079, 0.2742, 0.2747, 0.2644, 0.2668],
        [0.7016, 0.3125, 0.2759, 0.7002, 0.3038, 0.6950, 0.7618, 0.3326, 0.2852,
         0.6956, 0.3093, 0.7229, 0.2776, 0.2669, 0.3306, 0.3083, 0.2999, 0.2818,
         0.3060, 0.3004, 0.3178, 0.2970, 0.3162, 0.2796, 0.2923, 0.7181, 0.3088,
         0.3176, 0.3195, 0.7076, 0.2854, 0.2835]], device='cuda:0',
       grad_fn=<SigmoidBackward>)
